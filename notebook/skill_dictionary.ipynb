{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56010b83-61a6-4eba-a918-404dac75e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading spaCy model (this may take a moment) ---\n",
      "✅ spaCy model loaded.\n",
      "--- Loading the skills dataset from skills_list.csv ---\n",
      "--- Processing and applying advanced cleaning ---\n",
      "\n",
      "✅ Created a master dictionary with 50811 unique, ADVANCED CLEANED skills.\n",
      "\n",
      "Here is a sample of the new cleaned skills:\n",
      "['analyse', 'analyses', 'analysis', 'analysis activities', 'analysis and integration', 'analysis and presentation of data', 'analysis and refinement of system requirements', 'analysis and reporting', 'analysis and validation of physical models', 'analysis driven problem solving']\n",
      "\n",
      "✅ Final master skill dictionary saved to: ..\\data\\processed\\master_skills.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy # Import spaCy\n",
    "\n",
    "print(\"--- Loading spaCy model (this may take a moment) ---\")\n",
    "# Load the small English model. We only need its linguistic features.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy model loaded.\")\n",
    "\n",
    "\n",
    "print(\"--- Loading the skills dataset from skills_list.csv ---\")\n",
    "input_file_path = os.path.join('..', 'data', 'processed', 'skills_list.csv')\n",
    "df = pd.read_csv(input_file_path)\n",
    "df.dropna(subset=['job_skills'], inplace=True)\n",
    "\n",
    "print(\"--- Processing and applying advanced cleaning ---\")\n",
    "all_skills_set = set()\n",
    "\n",
    "# --- ADVANCED: Define rules using spaCy ---\n",
    "def is_valid_skill_advanced(skill_text, nlp_doc):\n",
    "    # Rule 1: Basic text filters (as before)\n",
    "    if not 2 <= len(skill_text) <= 50: return False\n",
    "    if re.search(r'[\\d$€£]', skill_text): return False\n",
    "    noise_phrases = ['salary', 'hourly', 'bonus', 'mindset', 'per hour', 'llc', 'inc', 'benefits', 'ability to']\n",
    "    if any(phrase in skill_text for phrase in noise_phrases): return False\n",
    "    \n",
    "    # Rule 2: Linguistic Filtering using spaCy\n",
    "    # A valid skill should not start with a verb (like 'work', 'lift') or be an entire clause.\n",
    "    # We check the Part-of-Speech (POS) tag of the first token.\n",
    "    # Good skills are usually nouns (NN, NNP) or noun chunks.\n",
    "    first_token_pos = nlp_doc[0].pos_\n",
    "    if first_token_pos not in ['NOUN', 'PROPN', 'ADJ']:\n",
    "        return False\n",
    "        \n",
    "    # Rule 3: Reject if it's identified as a geographic or political entity (GPE)\n",
    "    for ent in nlp_doc.ents:\n",
    "        if ent.label_ == 'GPE': # GPE = Geopolitical Entity (e.g., \"united states\")\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "# Loop through each row in the DataFrame's 'job_skills' column\n",
    "for skills_string in df['job_skills']:\n",
    "    skills_list = [skill.strip().lower() for skill in skills_string.split(',')]\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        cleaned_skill = re.sub(r'[\\\\\"\\'\\(\\)\\[\\]]', '', skill).strip()\n",
    "        \n",
    "        if cleaned_skill:\n",
    "            # Process the skill with spaCy to get its linguistic features\n",
    "            doc = nlp(cleaned_skill)\n",
    "            \n",
    "            # Only add the skill if it passes our new advanced validation\n",
    "            if is_valid_skill_advanced(cleaned_skill, doc):\n",
    "                all_skills_set.add(cleaned_skill)\n",
    "\n",
    "# Convert to a final sorted list\n",
    "master_skill_list = sorted(list(all_skills_set))\n",
    "\n",
    "print(f\"\\nCreated a master dictionary with {len(master_skill_list)} unique, ADVANCED CLEANED skills.\")\n",
    "print(\"\\nHere is a sample of the new cleaned skills:\")\n",
    "print(master_skill_list[1500:1510])\n",
    "\n",
    "# --- Save the final clean dictionary ---\n",
    "output_path = os.path.join('..', 'data', 'processed', 'master_skills.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\"skills\": master_skill_list}, f, indent=4)\n",
    "\n",
    "print(f\"\\nFinal master skill dictionary saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6830b-cc62-4788-bec8-8f6218b00591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
